{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "private_outputs": true,
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 📦 Packages and Basic Setup\n",
    "---"
   ],
   "metadata": {
    "id": "dxwN1PKH0y66"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9d6cFMQ0udH"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U rich\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from rich import print\n",
    "import tensorflow as tf\n",
    "from itertools import groupby\n",
    "from rich.progress import track\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "\n",
    "from typing import Callable, Optional, Tuple, Any, List\n",
    "\n",
    "# Experimental options\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.noop_elimination = True\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "options.experimental_deterministic = False\n",
    "options.threading.max_intra_op_parallelism = 1\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ⚙ Configuration\n",
    "GLOBAL_SEED = 42  # @param {type: \"number\"}\n",
    "NUM_CROPS = [2, 3]\n",
    "MIN_SCALE = [0.5, 0.14]\n",
    "MAX_SCALE = [1.0, 0.5]\n",
    "SIZE_CROPS = [224, 96]\n",
    "CROPS_FOR_ASSIGN = [0, 1]\n",
    "NUM_TRAINING_EPOCHS = 10  # @param {type: \"number\"}\n",
    "NUM_EVAL_EPOCHS = 100  # @param {type: \"number\"}\n",
    "TRAIN_BATCH_SIZE = 8  # @param {type: \"number\"}\n",
    "EVAL_BATCH_SIZE = 64  # @param {type: \"number\"}\n",
    "TEMPERATURE = 0.1  # @param {type: \"number\"}\n",
    "DECAY_STEPS = 1000  # @param {type: \"number\"}\n",
    "WEIGHT_DECAY = 1e-6  # @param {type: \"number\"}\n",
    "BASE_LR = 0.2  # @param {type: \"number\"}\n",
    "EVAL_LR = 0.02  # @param {type: \"number\"}\n",
    "\n",
    "\n",
    "# ============ Random Seed ============\n",
    "def seed_everything(seed=GLOBAL_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "seed_everything()"
   ],
   "metadata": {
    "id": "2W5EbcWR090S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Limit GPU memory growth\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ],
   "metadata": {
    "id": "lC925Hx0bfv7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 🆘 Utility Classes and Functions\n",
    "---"
   ],
   "metadata": {
    "id": "AnKXHwr01J5-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def random_apply(func: Callable, x: tf.Tensor, p: float) -> tf.Tensor:\n",
    "    \"\"\"Randomly apply the desired func to the input image\"\"\"\n",
    "    return tf.cond(\n",
    "        tf.less(\n",
    "            tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "            tf.cast(p, tf.float32),\n",
    "        ),\n",
    "        lambda: func(x),\n",
    "        lambda: x,\n",
    "    )\n",
    "\n",
    "\n",
    "def shuffle_zipped_output(a: Any, b: Any, c: Any, d: Any, e: Any) -> Tuple[Any]:\n",
    "    \"\"\"Shuffle the given inputs\"\"\"\n",
    "    listify = [a, b, c, d, e]\n",
    "    random.shuffle(listify)\n",
    "\n",
    "    return listify[0], listify[1], listify[2], listify[3], listify[4]\n",
    "\n",
    "\n",
    "def sinkhorn(\n",
    "    sample_prototype_batch: tf.Tensor, num_sinkhorn_iters: int = 3\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Perform sinkhorn normalization on the sample prototype batch\n",
    "    \"\"\"\n",
    "    Q = tf.transpose(tf.exp(sample_prototype_batch / 0.05))\n",
    "    Q /= tf.keras.backend.sum(Q)\n",
    "    K, B = tf.shape(Q)\n",
    "\n",
    "    u = tf.zeros_like(K, dtype=tf.float32)\n",
    "    r = tf.ones_like(K, dtype=tf.float32) / K\n",
    "    c = tf.ones_like(B, dtype=tf.float32) / B\n",
    "\n",
    "    for _ in range(num_sinkhorn_iters):\n",
    "        u = tf.keras.backend.sum(Q, axis=1)\n",
    "        Q *= tf.expand_dims((r / u), axis=1)\n",
    "        Q *= tf.expand_dims(c / tf.keras.backend.sum(Q, axis=0), 0)\n",
    "\n",
    "    final_quantity = Q / tf.keras.backend.sum(Q, axis=0, keepdims=True)\n",
    "    final_quantity = tf.transpose(final_quantity)\n",
    "\n",
    "    return final_quantity"
   ],
   "metadata": {
    "id": "CaWV-bp52gxJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🖖 Utilites for Data Augmentation"
   ],
   "metadata": {
    "id": "LicBqUSj1L2Q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def scale_image(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Convert all images to float32\"\"\"\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def scale_image_with_label(image: tf.Tensor, label: tf.Tensor) -> Tuple[tf.Tensor]:\n",
    "    \"\"\"Convert all images to float32\"\"\"\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return (image, label)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def gaussian_blur(\n",
    "    image: tf.Tensor, kernel_size: int = 23, padding: str = \"SAME\"\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Randomly apply Gaussian Blur to the input image\n",
    "\n",
    "    Reference: https://github.com/google-research/simclr/blob/master/data_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    sigma = tf.random.uniform((1,)) * 1.9 + 0.1\n",
    "    radius = tf.cast(kernel_size / 2, tf.int32)\n",
    "    kernel_size = radius * 2 + 1\n",
    "    x = tf.cast(range(-radius, radius + 1), tf.float32)\n",
    "    blur_filter = tf.exp(\n",
    "        -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.cast(sigma, tf.float32), 2.0))\n",
    "    )\n",
    "    blur_filter /= tf.reduce_sum(blur_filter)\n",
    "\n",
    "    # One vertical and one horizontal filter.\n",
    "    blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "    blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "    num_channels = tf.shape(image)[-1]\n",
    "    blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "    blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "    expand_batch_dim = image.shape.ndims == 3\n",
    "    if expand_batch_dim:\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        image, blur_h, strides=[1, 1, 1, 1], padding=padding\n",
    "    )\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "        blurred, blur_v, strides=[1, 1, 1, 1], padding=padding\n",
    "    )\n",
    "    if expand_batch_dim:\n",
    "        blurred = tf.squeeze(blurred, axis=0)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def color_jitter(image: tf.Tensor, s: float = 0.5) -> tf.Tensor:\n",
    "    \"\"\"Randomly apply Color Jittering to the input image\"\"\"\n",
    "    x = tf.image.random_brightness(image, max_delta=0.8 * s)\n",
    "    x = tf.image.random_contrast(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_saturation(x, lower=1 - 0.8 * s, upper=1 + 0.8 * s)\n",
    "    x = tf.image.random_hue(x, max_delta=0.2 * s)\n",
    "    x = tf.clip_by_value(x, 0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def color_drop(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Randomly convert the input image to GrayScale\"\"\"\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def random_resize_crop(\n",
    "    image: tf.Tensor,\n",
    "    min_scale: float,\n",
    "    max_scale: float,\n",
    "    crop_size: int,\n",
    "    label: Optional[tf.Tensor],\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Randomly resize and crop the input image\"\"\"\n",
    "    if crop_size == 224:\n",
    "        image_shape = 260\n",
    "        image = tf.image.resize(image, (image_shape, image_shape))\n",
    "    else:\n",
    "        image_shape = 160\n",
    "        image = tf.image.resize(image, (image_shape, image_shape))\n",
    "\n",
    "    # Get the crop size for given min and max scale\n",
    "    size = tf.random.uniform(\n",
    "        shape=(1,),\n",
    "        minval=min_scale * image_shape,\n",
    "        maxval=max_scale * image_shape,\n",
    "        dtype=tf.float32,\n",
    "    )\n",
    "    size = tf.cast(size, tf.int32)[0]\n",
    "\n",
    "    # Get the crop from the image\n",
    "    crop = tf.image.random_crop(image, (size, size, 3))\n",
    "    crop_resize = tf.image.resize(crop, (crop_size, crop_size))\n",
    "\n",
    "    return (crop_resize, label) if label is not None else crop_resize\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def augmentation_pipeline(image: tf.Tensor) -> tf.Tensor:\n",
    "    # Random flips\n",
    "    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n",
    "    # Randomly apply gausian blur\n",
    "    image = random_apply(gaussian_blur, image, p=0.5)\n",
    "    # Randomly apply transformation (color distortions) with probability p.\n",
    "    image = random_apply(color_jitter, image, p=0.8)\n",
    "    # Randomly apply grayscale\n",
    "    image = random_apply(color_drop, image, p=0.2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def apply_augmentation(\n",
    "    image: tf.Tensor, min_scale: float, max_scale: float, crop_size: int\n",
    ") -> tf.Tensor:\n",
    "    # Retrieve the image features\n",
    "    image = image[\"image\"]\n",
    "    # Scale the pixel values\n",
    "    image = scale_image(image)\n",
    "    # Random resized crops\n",
    "    image = random_resize_crop(image, min_scale, max_scale, crop_size, label=None)\n",
    "    # Color distortions & Gaussian blur\n",
    "    image = augmentation_pipeline(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def eval_augmentation(image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n",
    "    # Scale the pixel values\n",
    "    image, label = scale_image_with_label(image, label)\n",
    "    # random horizontal flip\n",
    "    image = random_apply(tf.image.flip_left_right, image, p=0.5)\n",
    "    # Random resized crops\n",
    "    image, label = random_resize_crop(\n",
    "        image,\n",
    "        min_scale=MIN_SCALE[0],\n",
    "        max_scale=MAX_SCALE[0],\n",
    "        crop_size=SIZE_CROPS[0],\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "    return image, label"
   ],
   "metadata": {
    "id": "0J74Z4VH0_-6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 💿 The Dataset\n",
    "---\n",
    "For the purposes of this example, we use the TF Flowers dataset.\n",
    "\n"
   ],
   "metadata": {
    "id": "Ff1jaGTm3O96"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tfds.disable_progress_bar()\n",
    "\n",
    "# Gather Flowers dataset\n",
    "train_ds, validation_ds = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:85%]\", \"train[85%:]\"],\n",
    ")"
   ],
   "metadata": {
    "id": "11ORswtg3TbM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🖖 Data Augmentation Pipeline"
   ],
   "metadata": {
    "id": "SkqF0-py3VCn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We create a Tuple because we have two loaders corresponding to each view\n",
    "trainloaders = tuple()\n",
    "\n",
    "for i, num_crop in enumerate(NUM_CROPS):\n",
    "    for _ in range(num_crop):\n",
    "        trainloader = train_ds.shuffle(1024).map(\n",
    "            lambda x: apply_augmentation(x, MIN_SCALE[i], MAX_SCALE[i], SIZE_CROPS[i]),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        )\n",
    "        trainloader = trainloader.with_options(options)\n",
    "        trainloaders += (trainloader,)"
   ],
   "metadata": {
    "id": "EndJFK7b3Vi5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ⚙️ Dataloader"
   ],
   "metadata": {
    "id": "sRHhkM-34a73"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# zip both the dataloaders together\n",
    "trainloader = tf.data.Dataset.zip(trainloaders)\n",
    "\n",
    "# final trainloader to be used for training\n",
    "trainloader = trainloader.batch(\n",
    "    TRAIN_BATCH_SIZE * strategy.num_replicas_in_sync\n",
    ").prefetch(AUTOTUNE)"
   ],
   "metadata": {
    "id": "d9kdlswP4aiC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ✍️ Model Architecture & Training\n",
    "---\n"
   ],
   "metadata": {
    "id": "KS-3y0Ar8AEU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏠 Building the network\n",
    "\n",
    "![](https://i.ibb.co/TtSW4Fd/figure-3.png)"
   ],
   "metadata": {
    "id": "jzA_C68L8CIT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SwAV(tf.keras.Model):\n",
    "    \"\"\"SwAV model class\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: Tuple[int, int] = (1024, 96),\n",
    "        projection_dim=10,\n",
    "        num_sinkhorn_iters: int = 3,\n",
    "        CROPS_FOR_ASSIGN: Tuple[int, int] = (0, 1),\n",
    "        NUM_CROPS: Tuple[int, int] = (2, 3),\n",
    "        TEMPERATURE: float = 0.1,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.projection_dim = projection_dim\n",
    "        self.CROPS_FOR_ASSIGN = CROPS_FOR_ASSIGN\n",
    "        self.NUM_CROPS = NUM_CROPS\n",
    "        self.TEMPERATURE = TEMPERATURE\n",
    "        self.num_sinkhorn_iters = num_sinkhorn_iters\n",
    "\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.projection = self.build_projection(self.units, self.projection_dim)\n",
    "\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"swav_loss\")\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"units\": self.units,\n",
    "            \"projection_dim\": self.projection_dim,\n",
    "            \"num_sinkhorn_iters\": self.num_sinkhorn_iters,\n",
    "            \"CROPS_FOR_ASSIGN\": self.CROPS_FOR_ASSIGN,\n",
    "            \"NUM_CROPS\": self.NUM_CROPS,\n",
    "            \"TEMPERATURE\": self.TEMPERATURE,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config, custom_objects=None) -> \"SwAV\":\n",
    "        return cls(**config)\n",
    "\n",
    "    @property\n",
    "    def metrics(self) -> list:\n",
    "        return [\n",
    "            self.loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def save_weights(\n",
    "        self,\n",
    "        filepath: str = \"artifacts/swav/\",\n",
    "        overwrite=True,\n",
    "        save_format=\"h5\",\n",
    "        options=None,\n",
    "    ) -> None:\n",
    "        self.encoder.save_weights(\"encoder.h5\")\n",
    "        self.projection.save_weights(\"projection.h5\")\n",
    "\n",
    "    def build_encoder(self) -> tf.keras.Model:\n",
    "        encoder_input = tf.keras.layers.Input((None, None, 3))\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            include_top=False, weights=None, input_shape=(None, None, 3)\n",
    "        )\n",
    "        base_model.trainable = True\n",
    "        representations = base_model(encoder_input, training=True)\n",
    "        encoder_output = tf.keras.layers.GlobalAveragePooling2D()(representations)\n",
    "        encoder = tf.keras.models.Model(\n",
    "            inputs=encoder_input, outputs=encoder_output, name=\"encoder\"\n",
    "        )\n",
    "        return encoder\n",
    "\n",
    "    def build_projection(self, units, projection_dim) -> tf.keras.Model:\n",
    "        inputs = tf.keras.layers.Input((2048,))\n",
    "        projection_1 = tf.keras.layers.Dense(units[0])(inputs)\n",
    "        projection_1 = tf.keras.layers.BatchNormalization()(projection_1)\n",
    "        projection_1 = tf.keras.layers.Activation(\"relu\")(projection_1)\n",
    "\n",
    "        projection_2 = tf.keras.layers.Dense(units[1])(projection_1)\n",
    "        projection_2_normalize = tf.math.l2_normalize(\n",
    "            projection_2, axis=1, name=\"projection\"\n",
    "        )\n",
    "\n",
    "        prototype = tf.keras.layers.Dense(\n",
    "            projection_dim, use_bias=False, name=\"prototype\"\n",
    "        )(projection_2_normalize)\n",
    "\n",
    "        return tf.keras.models.Model(\n",
    "            inputs=inputs, outputs=[projection_2_normalize, prototype]\n",
    "        )\n",
    "\n",
    "    def train_step(self, images: tf.Tensor) -> dict:\n",
    "        \"\"\"\n",
    "        References:\n",
    "\n",
    "        * https://github.com/facebookresearch/swav/blob/master/main_swav.py\n",
    "        * https://github.com/facebookresearch/swav/issues/19\n",
    "        * https://github.com/ayulockin/SwAV-TF\n",
    "        \"\"\"\n",
    "        im1, im2, im3, im4, im5 = images\n",
    "        inputs = [im1, im2, im3, im4, im5]\n",
    "        batch_size = inputs[0].shape[0]\n",
    "\n",
    "        # ============ create crop entries with same shape ... ============\n",
    "        crop_sizes = [inp.shape[1] for inp in inputs]  # list of crop size of views\n",
    "        unique_consecutive_count = [\n",
    "            len([elem for elem in g]) for _, g in groupby(crop_sizes)\n",
    "        ]  # equivalent to torch.unique_consecutive\n",
    "        idx_crops = tf.cumsum(unique_consecutive_count)\n",
    "\n",
    "        # ============ multi-res forward passes ... ============\n",
    "        start_idx = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            for end_idx in idx_crops:\n",
    "                concat_input = tf.stop_gradient(\n",
    "                    tf.concat(inputs[start_idx:end_idx], axis=0)\n",
    "                )\n",
    "                _embedding = self.encoder(\n",
    "                    concat_input\n",
    "                )  # get embedding of same dim views together\n",
    "                if start_idx == 0:\n",
    "                    embeddings = _embedding  # for first iter\n",
    "                else:\n",
    "                    embeddings = tf.concat(\n",
    "                        (embeddings, _embedding), axis=0\n",
    "                    )  # concat all the embeddings from all the views\n",
    "                start_idx = end_idx\n",
    "\n",
    "            projection, prototype = self.projection(\n",
    "                embeddings\n",
    "            )  # get normalized projection and prototype\n",
    "            projection = tf.stop_gradient(projection)\n",
    "\n",
    "            # ============ swav loss ... ============\n",
    "            loss = 0\n",
    "            for i, crop_id in enumerate(self.CROPS_FOR_ASSIGN):\n",
    "                with tape.stop_recording():\n",
    "                    out = prototype[batch_size * crop_id : batch_size * (crop_id + 1)]\n",
    "\n",
    "                    # get assignments\n",
    "                    q = sinkhorn(\n",
    "                        out, self.num_sinkhorn_iters\n",
    "                    )  # sinkhorn is used for cluster assignment\n",
    "\n",
    "                # cluster assignment prediction\n",
    "                subloss = 0\n",
    "                for v in np.delete(\n",
    "                    np.arange(np.sum(self.NUM_CROPS)), crop_id\n",
    "                ):  # (for rest of the portions compute p and take cross entropy with q)\n",
    "                    p = tf.nn.softmax(\n",
    "                        prototype[batch_size * v : batch_size * (v + 1)]\n",
    "                        / self.TEMPERATURE\n",
    "                    )\n",
    "                    subloss -= tf.math.reduce_mean(\n",
    "                        tf.math.reduce_sum(q * tf.math.log(p), axis=1)\n",
    "                    )\n",
    "                loss += subloss / tf.cast(\n",
    "                    (tf.reduce_sum(self.NUM_CROPS) - 1), tf.float32\n",
    "                )\n",
    "\n",
    "            loss /= len(self.CROPS_FOR_ASSIGN)  # type: ignore\n",
    "\n",
    "        # ============ backprop ... ============\n",
    "        variables = (\n",
    "            self.encoder.trainable_variables + self.projection.trainable_variables\n",
    "        )\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ],
   "metadata": {
    "id": "ZnFAWe0q-EzI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏃 Train !!"
   ],
   "metadata": {
    "id": "_8adaE2wOP6U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=BASE_LR, decay_steps=DECAY_STEPS\n",
    ")\n",
    "opt = tf.keras.optimizers.experimental.SGD(learning_rate=lr_decayed_fn)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = SwAV()\n",
    "    model.compile(optimizer=opt, run_eagerly=True)\n",
    "\n",
    "model.fit(\n",
    "    trainloader,\n",
    "    epochs=NUM_TRAINING_EPOCHS,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.BackupAndRestore(\n",
    "            \"artifacts/swav/checkpoints/\", save_freq=\"epoch\", delete_checkpoint=False\n",
    "        )\n",
    "    ],\n",
    ")"
   ],
   "metadata": {
    "id": "ZxMaqkZSOUib"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 👨🏻‍⚖️ Linear Evaluation\n",
    "---\n",
    "We use a linear evaluation protocol i.e., we train a linear classifier on top of the frozen representations of the ResNet-50 backbone pretrained with SwAV."
   ],
   "metadata": {
    "id": "Vtft7dBPce9u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "feature_backbone_urlpath = \"https://github.com/ayulockin/SwAV-TF/releases/download/v0.1.0/feature_backbone_10_epochs.h5\"\n",
    "feature_backbone_weights = tf.keras.utils.get_file(\n",
    "    \"swav_feature_weights\", feature_backbone_urlpath\n",
    ")"
   ],
   "metadata": {
    "id": "YLXFLRvNhIJa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ⚙️ Dataloader for Linear Evaluation"
   ],
   "metadata": {
    "id": "DWeOTMzwck3S"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Gather Flowers dataset\n",
    "train_ds, validation_ds = tfds.load(\n",
    "    \"tf_flowers\", split=[\"train[:85%]\", \"train[85%:]\"], as_supervised=True\n",
    ")\n",
    "\n",
    "eval_trainloader = (\n",
    "    train_ds.shuffle(1024)\n",
    "    .map(eval_augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(EVAL_BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_testloader = (\n",
    "    validation_ds.map(scale_image_with_label, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(EVAL_BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "eval_trainloader = eval_trainloader.with_options(options)"
   ],
   "metadata": {
    "id": "OVkW7Oz3cmUz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏠 Building the network"
   ],
   "metadata": {
    "id": "utged4nTfLVy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_linear_classifier() -> tf.keras.Model:\n",
    "    # input placeholder\n",
    "    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "    # get swav model architecture\n",
    "    base_model = SwAV()\n",
    "    feature_backbone = base_model.build_encoder()\n",
    "    # load trained weights\n",
    "    feature_backbone.load_weights(feature_backbone_weights)\n",
    "    feature_backbone.trainable = False\n",
    "    x = feature_backbone(inputs, training=False)\n",
    "    outputs = tf.keras.layers.Dense(5, activation=\"softmax\")(x)\n",
    "    linear_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return linear_model"
   ],
   "metadata": {
    "id": "r2lm3RhRfOnk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 🏃 Evaluation !!"
   ],
   "metadata": {
    "id": "r7Sc4P27fwe-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with strategy.scope():\n",
    "    evaluation_model = get_linear_classifier()\n",
    "    evaluation_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"acc\"],\n",
    "        optimizer=tf.keras.optimizers.experimental.SGD(learning_rate=lr_decayed_fn),\n",
    "    )\n",
    "\n",
    "evaluation_model.summary()"
   ],
   "metadata": {
    "id": "n4ieVcQ5futp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# callback for early stopping\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, verbose=2, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# train the model meant for evaluation\n",
    "evaluation_model.fit(\n",
    "    eval_trainloader, epochs=NUM_EVAL_EPOCHS, callbacks=[early_stopper]\n",
    ")"
   ],
   "metadata": {
    "id": "z3xI3CBUgC65"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluation_model.evaluate(eval_testloader)"
   ],
   "metadata": {
    "id": "WvLc6jdzg64u"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
